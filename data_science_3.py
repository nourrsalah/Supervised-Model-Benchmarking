# -*- coding: utf-8 -*-
"""Data Science 3

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r0VsboFv4ftOpf-pnxmsIg4kgN2zATnQ

Team Member 1: Mazen Ahmed 13005132
Team Member 2: Nour salah 13001528
Team Member 3: Abdelrahman Farajallah 13003482
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, AgglomerativeClustering, Birch
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, davies_bouldin_score

#load the dataset
df = pd.read_csv('Mall_Customers.csv')
df.head(10)

df.info()

print(df.isnull().sum())

# Drop 'CustomerID'
df.drop('CustomerID', axis=1, inplace=True)

# Encode 'Gender' using LabelEncoder
label_encoder = LabelEncoder()
df['Genre'] = label_encoder.fit_transform(df['Genre'])  # Female: 0, Male: 1

# Feature Scaling
S = StandardScaler()
sdata = S.fit_transform(df)

#PCA to reduce the dataset to 2 dimensions
pca = PCA(n_components=2)
pca_data = pca.fit_transform(sdata)

# Initialize an empty list to store the inertia (within-cluster sum of squares) for each value of k
inertia = []

# Define the range of k values to evaluate (from 1 to 10 clusters)
k_range = range(1, 11)

# Loop through each k in the specified range
for k in k_range:
    # Initialize the KMeans model with the current number of clusters (k)
    # random_state=42 ensures results are reproducible
    kmeans = KMeans(n_clusters=k, random_state=42)

    # Fit the KMeans model to the scaled data (sdata)
    kmeans.fit(sdata)

    # Store the inertia value (WCSS) for this k
    # Lower inertia means tighter clusters, but we look for the "elbow" point
    inertia.append(kmeans.inertia_)

# Create a new figure with a specific size (8 inches wide, 4 inches tall)
plt.figure(figsize=(8, 4))

# Plot the number of clusters (k) on the x-axis against the inertia values on the y-axis
# 'marker="o"' adds a circular marker at each data point
plt.plot(k_range, inertia, marker='o')

# Label the x-axis to indicate the number of clusters
plt.xlabel('Number of Clusters (k)')

# Label the y-axis to indicate the inertia (within-cluster sum of squares)
plt.ylabel('Inertia')

# Set the title of the plot to explain what it represents
plt.title('Elbow Method to Find Optimal k')

# Add grid lines for better readability of the plot
plt.grid(True)

# Display the plot
plt.show()

optimal_k = 5

# Initialize the KMeans model with the chosen optimal number of clusters
# random_state=42 ensures reproducibility
kmeans = KMeans(n_clusters=optimal_k, random_state=42)

# Fit the model to the scaled data and predict cluster labels
# Each data point is assigned to one of the k clusters
kmeans_labels = kmeans.fit_predict(sdata)

# Create a new figure for the PCA-based cluster visualization
plt.figure(figsize=(6, 5))

# Plot the PCA-reduced data, coloring each point based on its assigned cluster label
# pca_data[:, 0] is the first principal component (x-axis)
# pca_data[:, 1] is the second principal component (y-axis)
# cmap='Set2' provides distinct, visually pleasing colors for clusters
plt.scatter(pca_data[:, 0], pca_data[:, 1], c=kmeans_labels, cmap='Set2')

# Set plot title and axis labels
plt.title('K-Means Clustering (PCA Projection)')
plt.xlabel('PCA 1')
plt.ylabel('PCA 2')

# Show the plot
plt.show()

# Initialize the Agglomerative (Hierarchical) Clustering model with the optimal number of clusters
hierarchical = AgglomerativeClustering(n_clusters=optimal_k)

# Fit the model to the scaled data and predict cluster labels
# Each data point is assigned to one of the clusters based on hierarchical linkage
hierarchical_labels = hierarchical.fit_predict(sdata)

# Create a new figure for visualizing hierarchical clustering in PCA space
plt.figure(figsize=(6, 5))

# Plot the PCA-reduced data, coloring each point based on its assigned cluster from hierarchical clustering
# Using 'Set1' colormap for distinct cluster colors
plt.scatter(pca_data[:, 0], pca_data[:, 1], c=hierarchical_labels, cmap='Set1')

# Set the title and axis labels
plt.title('Hierarchical Clustering (PCA Projection)')
plt.xlabel('PCA 1')
plt.ylabel('PCA 2')

# Show the plot
plt.show()

gmm = GaussianMixture(n_components=optimal_k, random_state=42)
gmm_labels = gmm.fit_predict(sdata)

plt.figure(figsize=(6, 5))
plt.scatter(pca_data[:, 0], pca_data[:, 1], c=gmm_labels, cmap='Set3')
plt.title('GMM Clustering (PCA Projection)')
plt.xlabel('PCA 1')
plt.ylabel('PCA 2')
plt.show()

# STEP E3: BIRCH CLUSTERING

# Initialize the BIRCH clustering model with the optimal number of clusters
# BIRCH is efficient for large datasets and incrementally builds a clustering feature tree
birch = Birch(n_clusters=optimal_k)

# Fit the model to the scaled data and get the cluster labels
birch_labels = birch.fit_predict(sdata)

# Create a new figure for plotting BIRCH clusters in 2D PCA space
plt.figure(figsize=(6, 5))

# Plot the PCA-transformed data with cluster coloring from BIRCH
# cmap='tab10' provides a colorful and distinct set of cluster colors
plt.scatter(pca_data[:, 0], pca_data[:, 1], c=birch_labels, cmap='tab10')

# Add plot title and axis labels
plt.title('BIRCH Clustering (PCA Projection)')
plt.xlabel('PCA 1')
plt.ylabel('PCA 2')

# Display the cluster plot
plt.show()

# STEP F: EVALUATION OF CLUSTERING METHODS

# Dictionary to store each clustering method and its corresponding predicted cluster labels
methods = {
    'KMeans': kmeans_labels,
    'Hierarchical': hierarchical_labels,
    'GMM': gmm_labels,
    'BIRCH': birch_labels
}

# Print the evaluation heading
print("\nEvaluation Metrics:")

# Loop through each method and compute evaluation metrics
for method, labels in methods.items():
    # Silhouette Score: measures how similar a point is to its own cluster vs. other clusters
    # Range: [-1, 1] — higher is better
    sil_score = silhouette_score(sdata, labels)

    # Davies-Bouldin Index: evaluates intra-cluster similarity and inter-cluster differences
    # Lower is better
    db_score = davies_bouldin_score(sdata, labels)

    # Print evaluation metrics for each clustering algorithm
    print(f"{method} => Silhouette Score: {sil_score:.3f}, Davies-Bouldin Index: {db_score:.3f}")

"""**Interpretation**

We used four different clustering methods — KMeans, Hierarchical, Gaussian Mixture Model (GMM), and BIRCH — to group customers based on their age, income, and spending habits. Here’s what we found:

**KMeans** created clear and well-separated groups, with the best scores. This means the clusters are tight and distinct, so KMeans works well for this data.

**Hierarchical** clustering also found useful groups, but the clusters overlapped a bit more. It’s helpful if you want to see how groups relate to each other.

**GMM** is a bit different because it gives each customer a chance to belong to more than one group. Its clusters look similar to KMeans and performed well too.

**BIRCH** is good for handling large data quickly and made clusters close to the others, but its groups were a little less tight.

Business takeaway:
The clusters likely show groups like high spenders, moderate spenders, and low spenders. Retailers can use this info to:

Give premium offers to big spenders.

Offer loyalty deals to regular customers.

Send discounts to budget-conscious shoppers.

In short: KMeans is the best fit here because it clearly separates customers and is easy to understand. Using PCA for visuals and scoring the models confirms that our clustering makes sense.
"""